{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Sophia Hart, MS, MEng, MS<center>\n",
    "## <center>sophia.t.hart@gmail.com<center>\n",
    "# <center>TensorFlow Final Project<center>\n",
    "## <center>Prof. Alexander I. Iliev, PhD<center>\n",
    "## <center>UC Berkeley, Spring 2022<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal and Data:\n",
    "Wildfire is a davastating problem in California due to climate change. Last year, in 2021, there were 185 wildfires in California. Wildfires are usually detected by satellite and camera towel images. Human looking at these images causes fatiuge and error.\n",
    "\n",
    "In this project, I downloaded fire and nonfire images from the internet, and built a convolution neural network by transfer learning with the pretrained Xception model to classify if an image shows there is or there is no wildfire. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ce233a7ed2a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import glob\n",
    "import random\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide all jpeg files into training and testing filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gather all the files from the fire and nonfire directories\n",
    "# Some file names are jpg and some are jpeg\n",
    "paths = glob.glob(\"./*fire/*.jp*g\")\n",
    "\n",
    "# Shuffle them because top is fire and bottom is nonfire images\n",
    "random.shuffle(paths)\n",
    "\n",
    "print(\"Total number of images \", len(paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames for train and test sets\n",
    "\n",
    "train_filenames = []\n",
    "test_filenames = []\n",
    "i = 0\n",
    "\n",
    "for filename in paths:  \n",
    "    \n",
    "     # Send 25% (1 out of 4) of the images to testing set, the rest to training set\n",
    "    if i % 4 == 0:\n",
    "        test_filenames.append(filename)\n",
    "    else: train_filenames.append(filename) \n",
    "    \n",
    "    i += 1\n",
    "      \n",
    "print(\"Original number of files for training set: \", len(train_filenames), train_filenames)\n",
    "print(\"Original number of files for testing set: \", len(test_filenames), test_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create training and testing dataset\n",
    "# data_filenames is train_filenames or test_filenames.\n",
    "\n",
    "def create_dataset(data_filenames):\n",
    "    \n",
    "    # Create empty list for images and target (ie label: 1 is fire, 0 is nonfire)\n",
    "    # The lists will be converted into tensor at the end\n",
    "    image_dataset = []\n",
    "    target = []    \n",
    "    a = 0    # index for plotting subplot\n",
    "    axes = []\n",
    "    rows = 10\n",
    "    cols = 10\n",
    "    fig = plt.figure(figsize = (50, 50))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    for filename in data_filenames:\n",
    "        # Read each image file\n",
    "        image_file = tf.io.read_file(filename)\n",
    "            \n",
    "        # try/catch if a file is not jpeg \n",
    "        try:\n",
    "            image = tf.image.decode_jpeg(image_file)\n",
    "        except:\n",
    "            print(\"Not a jpeg file: \", image_file)\n",
    "        \n",
    "        # Split filename by \"/\". The category of fire or nonfire is the second element on the filename\n",
    "        category = filename.split(\"/\")[1]\n",
    "        if category == \"fire\":\n",
    "            label = 1\n",
    "        else: label = 0\n",
    "      \n",
    "        # Resize image, Xception model expect size 224x224 images\n",
    "        # Can't do resize_with_crop_or_pad, not compatible with Xception preprocess_input()\n",
    "        #resize_image = tf.image.resize_with_crop_or_pad(image, 224, 224)\n",
    "        resize_image = tf.image.resize(image, [224, 224])\n",
    "   \n",
    "        # Xception model needs run data through preprocess_input function\n",
    "        final_image = keras.applications.xception.preprocess_input(resize_image)\n",
    "        \n",
    "        # Write image and label to datset\n",
    "        image_dataset.append(np.array(final_image))\n",
    "        target.append(label)\n",
    "    \n",
    "        # Display image with title (fire or nonfire category that's associated)\n",
    "        axes.append(fig.add_subplot(rows,cols, a+1))\n",
    "        plt.imshow(resize_image/255)\n",
    "        plt.title(category, fontsize=50)\n",
    "        plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]) # get rid of ticks on x-axis and y-axis\n",
    "        a += 1\n",
    "      \n",
    "       \n",
    "        # Below is different data augmentation to increase dataset counts\n",
    "        \n",
    "        # flip left-right\n",
    "        image_flipLR = tf.image.flip_left_right(resize_image)\n",
    "        final_image = keras.applications.xception.preprocess_input(image_flipLR)\n",
    "        # Write image and label to datset, still the same label\n",
    "        image_dataset.append(np.array(final_image))\n",
    "        target.append(label)\n",
    "\n",
    "        # flipt up-down\n",
    "        image_flipUD = tf.image.flip_up_down(resize_image)\n",
    "        final_image = keras.applications.xception.preprocess_input(image_flipUD)\n",
    "        image_dataset.append(np.array(final_image))\n",
    "        target.append(label)\n",
    "    \n",
    "        # flipt left-right then up-down\n",
    "        image_flipLRUD = tf.image.flip_up_down(image_flipLR)\n",
    "        final_image = keras.applications.xception.preprocess_input(image_flipLRUD)\n",
    "        image_dataset.append(np.array(final_image))\n",
    "        target.append(label)\n",
    "        \n",
    "    return image_dataset, target\n",
    "             \n",
    "# Call the create_dataset() function to create train and test datasets\n",
    "train_image_dataset, train_target = create_dataset(train_filenames)\n",
    "\n",
    "test_image_dataset, test_target = create_dataset(test_filenames)\n",
    "\n",
    "# Convert train_data from a list to a tensor\n",
    "train_image_dataset = tf.convert_to_tensor(train_image_dataset, dtype=tf.float32)\n",
    "print('Training image dataset shape: ', train_image_dataset.shape)\n",
    "\n",
    "# Do the same for train_target\n",
    "train_target = tf.convert_to_tensor(train_target, dtype=tf.uint8)\n",
    "\n",
    "# Convert test_data from a list to tensor\n",
    "test_image_dataset = tf.convert_to_tensor(test_image_dataset, dtype=tf.float32)\n",
    "print('Testing image dataset shape: ', test_image_dataset.shape)\n",
    "\n",
    "# Do the same for test_target\n",
    "test_target = tf.convert_to_tensor(test_target, dtype=tf.uint8)\n",
    "\n",
    "\n",
    "# print(test_image_dataset)\n",
    "# --> output of this looks right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save datasets into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2cfff33739be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save datasets into a file for later usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_image_dataset.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_image_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_target.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_image_dataset.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Save datasets into a file for later usage\n",
    "\n",
    "np.save('train_image_dataset.npy', train_image_dataset)\n",
    "np.save('train_target.npy', train_target)\n",
    "np.save('test_image_dataset.npy', test_image_dataset)\n",
    "np.save('test_target.npy', test_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets from saved files\n",
    "\n",
    "train_image_dataset = np.load(\"train_image_dataset.npy\")\n",
    "print(\"Shape of train data: \", train_image_dataset.shape)\n",
    "print(type(train_image_dataset))\n",
    "\n",
    "test_image_dataset = np.load(\"test_image_dataset.npy\")\n",
    "print(\"Shape of test data: \", test_image_dataset.shape)\n",
    "\n",
    "train_target = np.load(\"train_target.npy\")\n",
    "print(\"Shape of train target\", train_target.shape)\n",
    "\n",
    "test_target = np.load(\"test_target.npy\")\n",
    "print(\"Shape of test target\", test_target.shape)\n",
    "\n",
    "# Convert train_data from a list to a tensor\n",
    "train_image_dataset = tf.convert_to_tensor(train_image_dataset, dtype=tf.float32)\n",
    "print('Training image dataset shape: ', train_image_dataset.shape)\n",
    "print(type(train_image_dataset))\n",
    "print(train_image_dataset.dtype)\n",
    "\n",
    "# Do the same for train_target\n",
    "train_target = tf.convert_to_tensor(train_target, dtype=tf.uint8)\n",
    "print(type(train_target))\n",
    "\n",
    "# Convert test_data from a list to tensor\n",
    "test_image_dataset = tf.convert_to_tensor(test_image_dataset, dtype=tf.float32)\n",
    "print('Testing image dataset shape: ', test_image_dataset.shape)\n",
    "print(type(test_image_dataset))\n",
    "print(test_image_dataset.dtype)\n",
    "\n",
    "# Do the same for test_target\n",
    "test_target = tf.convert_to_tensor(test_target, dtype=tf.uint8)\n",
    "print(type(test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model for Transfer Learning\n",
    "### Using Xception model from ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Xception as the base model\n",
    "\n",
    "base_model = keras.applications.xception.Xception(weights = 'imagenet', include_top=False)\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "n_classes = 2    # fire and nonfire\n",
    "\n",
    "# Output from the base_model\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "\n",
    "# Only two classes for softmax\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "\n",
    "# Combine base_model and softmax for our model\n",
    "model = keras.Model(inputs = base_model.input, outputs = output)\n",
    "\n",
    "# Trainable variables\n",
    "print(\"Trainable variables \", len(model.trainable_variables))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for tensorboard\n",
    "\n",
    "# Create log directory\n",
    "import os\n",
    "logdir = './logs/'\n",
    "\n",
    "os.makedirs(logdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous event files\n",
    "!rm -rf ./logs/train/event*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = keras.callbacks.TensorBoard(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the weights of the base model, ie not trainable\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile model. \n",
    "# Have to use sparse_categorical_crossentropy for compile to work\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.01)\n",
    "\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", \n",
    "              optimizer = optimizer, \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_image_dataset, \n",
    "    train_target, \n",
    "    epochs = 5,\n",
    "    callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ./logs/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show accuracy and loss with each epoch in TensorBoard\n",
    "\n",
    "%tensorboard --logdir ./logs/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# Predictions is a matrix. Row is the instance, column is the output\n",
    "predictions = model.predict(x=test_image_dataset, batch_size=60)\n",
    "print(predictions)\n",
    "\n",
    "# Use argmax to pull out the column with highest probability, \n",
    "# which is the most probable class\n",
    "predictions = tf.argmax(predictions, axis=-1)\n",
    "print(\"Target: \", test_target)\n",
    "print(\"Predict \", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions and test_target tensors to calculate accuracy\n",
    "\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state(predictions, test_target)\n",
    "accuracy = m.result().numpy()\n",
    "\n",
    "print(\"Prediction accurcay is: \", accuracy)\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"Confusion Matrix below: \")\n",
    "\n",
    "tf.math.confusion_matrix(test_target, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-tuning: \n",
    "### 1. Unfreeze top layers of Xception base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve model by unfreezing top layers of the base model, \n",
    "# set weights to trainable \n",
    "\n",
    "# Total layers of base model\n",
    "print(\"Number of layers in base model \", len(base_model.layers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze all layers then freeze top layers\n",
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune from this layer ownward\n",
    "fine_tune_at = 120\n",
    "\n",
    "# Freeze all layers before the fine-tune-at layer\n",
    "for layer in base_model.layers[: fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Now trainable variables in base model\n",
    "print(\"Trainable Variables in base_model \", len(base_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "# Output from the base_model\n",
    "avg2 = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "\n",
    "# Only two classes for softmax\n",
    "output2 = keras.layers.Dense(n_classes, activation=\"softmax\")(avg2)\n",
    "\n",
    "# Combine base_model and softmax for our model\n",
    "model2 = keras.Model(inputs = base_model.input, outputs = output2)\n",
    "\n",
    "# Trainable variables\n",
    "print(\"Trainable variables \", len(model2.trainable_variables))\n",
    "\n",
    "# Compile model. Use a slower learning rate and smaller dacay for learning rate\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.01)\n",
    "model2.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "             optimizer = optimizer,\n",
    "             metrics = [\"accuracy\"])\n",
    "\n",
    "# Tensorboard\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(logdir)\n",
    "\n",
    "# Train model\n",
    "# Run less epoch (4) because it's computational expansive when unfreeze top layers with more images\n",
    "history = model2.fit(train_image_dataset, \n",
    "                    train_target, \n",
    "                    epochs=3, \n",
    "                    callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# Predictions is a matrix. Row is the instance, column is the output\n",
    "predictions = model2.predict(x=test_image_dataset, batch_size=60)\n",
    "predictions = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "print(\"Target: \", test_target)\n",
    "print(\"Predict \", predictions)\n",
    "\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state(predictions, test_target)\n",
    "accuracy = m.result().numpy()\n",
    " \n",
    "print(\"Prediction accurcay is: \", accuracy)\n",
    "\n",
    "tf.math.confusion_matrix(test_target, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-tuning: \n",
    "### 2. Add a dropout layer to Xception base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "base_model = keras.applications.xception.Xception(weights = 'imagenet', include_top=False)\n",
    "\n",
    "n_classes = 2    # fire and nonfire\n",
    "\n",
    "# Connect base model\n",
    "avg3 = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "\n",
    "# Connect dropout3 \n",
    "dropout3 = keras.layers.Dropout(0.5)(avg3) # Also tried dropout rate = 0.25 and 0.8 (similar results)\n",
    "\n",
    "# Only two classes for softmax\n",
    "output3 = keras.layers.Dense(n_classes, activation=\"softmax\")(dropout3)\n",
    "\n",
    "# Combine base_model and softmax for our model\n",
    "model3 = keras.Model(inputs = base_model.input, outputs = output3)\n",
    "\n",
    "# Trainable variables\n",
    "print(\"Trainable variables \", len(model3.trainable_variables))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers again\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# Compile the model and start training\n",
    "\n",
    "# Compile model. Use large learning rate at the begining\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.01)\n",
    "model3.compile(loss = \"sparse_categorical_crossentropy\", \n",
    "              optimizer = optimizer, \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model3.fit(\n",
    "    train_image_dataset, \n",
    "    train_target, \n",
    "    epochs = 5,\n",
    "    callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# Predictions is a matrix. Row is the instance, column is the output\n",
    "predictions = model3.predict(x=test_image_dataset, batch_size=60)\n",
    "predictions = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "print(\"Target: \", test_target)\n",
    "print(\"Predict \", predictions)\n",
    "\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state(predictions, test_target)\n",
    "accuracy = m.result().numpy()\n",
    " \n",
    "print(\"Prediction accurcay is: \", accuracy)\n",
    "\n",
    "tf.math.confusion_matrix(test_target, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-tuning:\n",
    "### 3. Change opimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "n_classes = 2    # fire and nonfire\n",
    "\n",
    "# Output from the base_model\n",
    "avg4 = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "\n",
    "# Only two classes for softmax\n",
    "output4 = keras.layers.Dense(n_classes, activation=\"softmax\")(avg4)\n",
    "\n",
    "# Combine base_model and softmax for our model\n",
    "model4 = keras.Model(inputs = base_model.input, outputs = output4)\n",
    "\n",
    "# Trainable variables\n",
    "print(\"Trainable variables \", len(model4.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the weights of the base model, ie not trainable\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile model. Use large learning rate at the begining\n",
    "optimizer = keras.optimizers.Adam()\n",
    "optimizer.learning_rate = 0.01\n",
    "\n",
    "model4.compile(loss = \"sparse_categorical_crossentropy\", \n",
    "              optimizer = optimizer, \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model4.fit(\n",
    "    train_image_dataset, \n",
    "    train_target, \n",
    "    epochs = 5,\n",
    "    callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# Predictions is a matrix. Row is the instance, column is the output\n",
    "predictions = model4.predict(x=test_image_dataset, batch_size=28)\n",
    "predictions = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "print(\"Target: \", test_target)\n",
    "print(\"Predict \", predictions)\n",
    "\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state(predictions, test_target)\n",
    "accuracy = m.result().numpy()\n",
    " \n",
    "print(\"Prediction accurcay is: \", accuracy)\n",
    "\n",
    "tf.math.confusion_matrix(test_target, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: \n",
    "### Accuracy on unseen test image data for multiple runs\n",
    "\n",
    "\n",
    "| Total images | Run 1 accuracy | Run 2 | Run 3| Run 4 | Run 5 | Run 6 |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 108 | 80% | 83% |  NA |  NA | NA  | NA  |\n",
    "| 240 | 93% | 96% | 100% | 96% | NA | NA  |\n",
    "| 320 | 97% | 91% | 97% | 95% | NA | NA | \n",
    "| 400 | 96% | 95% | 87% | 98% | 97% | 99% - 100% | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results of accuracy on test images for multiple runs.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#x = [108, 240, 320, 400]\n",
    "x = [240, 320, 400]\n",
    "run1 = [93, 97.5, 96]\n",
    "run2 = [96.6, 91, 95]\n",
    "#x3 = [240, 320, 400]\n",
    "run3 = [100, 97.5, 87]\n",
    "run4 = [96.6, 95, 98]\n",
    "x5 = [400]\n",
    "run5 = [97]\n",
    "run6 = [100]\n",
    "\n",
    "plt.scatter(x, run1)\n",
    "plt.scatter(x, run2)\n",
    "plt.scatter(x, run3)\n",
    "plt.scatter(x, run4)\n",
    "plt.scatter(x5, run5)\n",
    "plt.scatter(x5, run6)\n",
    "\n",
    "plt.xlabel(\"Total Images\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", fontsize=20)\n",
    "plt.title(\"Accuracy on Test Images\", fontsize=30)\n",
    "\n",
    "# Average accuracy across 14 runs for 240, 320 and 400 images (exclude data with only 108 images)\n",
    "accuracy = [ 93, 97.5, 96, 96.6, 91, 95, 100, 97.5, 87, 96.6, 95, 98, 97, 100]\n",
    "avg = round(sum(accuracy) / len(accuracy), 2)\n",
    "print(\"Average accuracy across 12 runs is \", avg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "\n",
    "**Creating Dataset** \n",
    "\n",
    "At first, I attempted to create TFRecord and extract data from it. However, I encountered dimensionality mismatch when extracted each example for training. So I had this alternative method. \n",
    "\n",
    "There are two folders with fire and nonfire images, each has 50 images to balance the two classes. After shuffling, 25% of the images were sent for testing and 75% for training. I wrote a create_dataset() function to convert the images into nested list which is then converted into tensor for dataset. The labels/targets corresponding to the images are saved into another file with the same order as the images. Each image is resized to (224, 224). The channels are preserved because color is important in detecting the wildfire. Data augmentation is applied by flipping images left-right and up-down to quadruple into 400 images.\n",
    "\n",
    "**Model**\n",
    "\n",
    "Deep convolution neural network is used for the model.  I built my model base on the pretrained Xception base model (proposed in 2016 by Francois Chollet, author of Keras), that has 132 layers, and then I added 2 or 3 layers on top of it. The weights are extracted from imagenet training and they are set as non-trainable. Xception model has depthwise separable convolution layers, which assume spatial patterns and cross-channel patterns can be modeled separately [1]. This saves computation time.   \n",
    "\n",
    "**Results**\n",
    "\n",
    "I graduatlly increase the data size, 108, 240, 320 and 400 total images after augmentation. The table and plot above show the best accuracy on test images among 4 different models-- initial model and three fine-tuned models. The accuracies were above 90%, exception one outlier at 87% for 400 images but 5 others times fall between 95% - 100%.  The outlier could be because there are one or two images that are very ambigeous that got into the test set, this becomes 4 or 8 images after augmentation. The accuracy for 108 images are low, but they were just quick test that my code works when I was developing models.\n",
    "\n",
    "I tried three different ways to fine tune the model: 1. unfreezed the top layers, 2. added a dropout layer at the top layer, 3. changed the optimizer. Out of at least 12 times I ran the program, different strategies can do better or worse than the initial model.\n",
    "\n",
    "The more data I have, the wider distribution the data can be, so it is not black-and-white (fire or nonfire). The model is robus enough to give excellent predictions. Thanks to the Xception model that I was able to transfer the learning. Computer vision with pretrained model is amazing in accuracy and simplicity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1. Geron, Aurelien, Hands-on Machine Learning with SciKit-Learn, Keras & TensorFlow, 2nd Ed, O'Reilly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
